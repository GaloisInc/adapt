adapt {
  runflow = "quine" // See: Application.scala

  quine {
    quineactorparallelism = 3
    inmemsoftlimit = 2000
    inmemhardlimit = 4000
    thishost = "172.31.2.206"
    hosts = [
      {     // Pi 1
        ip = "172.31.2.201"
        shardcount = 3
        namespaces = ["trace-1"]
      }, {  // Pi 2
        ip = "172.31.2.202"
        shardcount = 3
        namespaces = ["trace-2"]
      }, {  // Pi 3
        ip = "172.31.2.203"
        shardcount = 3
        namespaces = ["trace-3"]
      }, {  // Pi 4
        ip = "172.31.2.204"
        shardcount = 3
        namespaces = ["trace-4"]
      }, {  // Pi 5
        ip = "172.31.2.205"
        shardcount = 3
        namespaces = ["trace-5"]
      }, {  // Pi 6
        ip = "172.31.2.206"
        shardcount = 3
        namespaces = ["trace-6"]
      }, {  // Pi 7
        ip = "172.31.2.207"
        shardcount = 3
        namespaces = ["trace-7"]
      }
    ]
    ppmobservationbuffer = 12000
  }

  publishadmintokafka = no
  kafkaadmsources = []
  numberadmtopicsinkafka = 0

  ingest {
    // Host stuff
    hosts = [
      
    ]

    quitafteringest = no
    logduplicates = no
    produce = produceadm
  }

  runtime {
    webinterface = "0.0.0.0"
    port = 8080
    apitimeout = 601
    dbkeyspace = "neo4j"    // "neo4j" or "titan" is the default value if keyspace is never set
    neo4jkeyspace = ${adapt.runtime.dbkeyspace}
    neo4jfile = ${adapt.runtime.neo4jkeyspace}.db  // "neo4j.db"
    systemname = "Engagement5"
    quitonerror = no
    logfile = "log.json.txt"
  }

  env {
    kafkabootstrap = "adapt-2.dev.galois.com:9092"
    truststorepath = "/var/private/ssl/kafka.client.truststore.jks"
    trustpass = "TransparentComputing"
    keystorepath = "/var/private/ssl/kafka.client.keystore.jks"
    keypass = "TransparentComputing"
    sslkey = "TransparentComputing"
    //    kafkasslenabled = yes
  }

  // NOTE: these settings are only used if `adapt.ingest.produceadm` is `yes`
  adm {
    // Maximum time hop (in seconds) in observed tip-of-the-stream time (does not apply to TimeMarkers) that is allowed
    // when assigning timestamps to nodes in the graph
    maxtimejumpsecs = 1000000

    // When to expire CdmUUIDs in the UUID remap step expiry
    cdmexpiryseconds = 300
    cdmexpirycount = 100000

    // When to expire event chains in the event resolution step
    maxeventsmerged = 100    // Maximum number of CDM events that can be merged into a single ADM event
    eventexpirysecs = 10
    eventexpirycount = 10000

    // Elements in the LRU cache in the dedup step
    dedupedgecachesize = 200000

    uuidremappershards = 4           // Setting this to 0 uses the old (pre-sharding) mechanism.
    cdm2cdmlrucachesize =  50000
    cdm2admlrucachesize = 150000

    // Hack to keep down the size of mapDB - assumes that edges in the initial CDM data never point to event nodes
    ignoreeventremaps = true

    // Location of MapDB file DB. Uncomment this to have information about maps persisted on shutdown.
    mapdb = admMapProxy.db  // leaving this absent will create a temporary file instead
    mapdbbypasschecksum = no  // don't change this unless you are desperate and know what you are doing
    mapdbtransactions = no
  }



  ppm {
    //    saveintervalseconds = 1200
    pluckingdelay = 10000
    basedir = "ppm_e5/"   // MUST end in a slash
    eventtypemodelsdir = ${adapt.ppm.basedir}iforest/   // MUST end in a slash
    loadfilesuffix = "-training" // ${adapt.env.ta1}
    savefilesuffix = ${adapt.ppm.loadfilesuffix}-saved

    shouldloadppmtrees = yes
    shouldloadalarms = no
    shouldloadlocalprobabilitiesfromalarms = yes
    shouldloadppmpartialobservationaccumulators = no

    shouldsaveppmtrees = yes
    shouldsavealarms = yes
    shouldsaveppmpartialobservationaccumulators = no

    rotatescriptpath = "" // /home/darpa/rotate-components.sh"

    components {
      events = ${adapt.ppm.basedir}event-components.json
      everything = ${adapt.ppm.basedir}everything-components.json
      pathnodes = ${adapt.ppm.basedir}pathnodes-components.json
      pathnodeuses = ${adapt.ppm.basedir}pathnodeuses-components.json
      releasequeue = ${adapt.ppm.basedir}releasequeue-components.json
    }

    iforestfreqminutes = 15
    iforesttrainingfile = train_iforest.csv
    iforesttrainingsavefile = train_iforest-UPDATED.csv
    iforestenabled = false

    computethresholdintervalminutes = 5
    alarmlppercentile = 0.01
  }

  // These options are only used when the runflow is "accept"
  test {
    web-ui = true // Don't exit until user hits CTRL-C
  }

  alarms {

    splunk {
      enabled = no
      token = 4265eb49-40ff-4bb1-a096-e9f4a51e6d17
      host = "ta2-adapt-7"
      port = 8088
      detailed-reporting-period-seconds = 900
      realtime-reporting-period-seconds = 1
      percent-process-instances-to-take = 0.1
      maxbufferlength = 500
    }

    logging {
      enabled = true
      fileprefix = "alarms-sent-to-splunk"
    }

    console {
      enabled = false
    }

    gui {
      enabled = false
    }
  }
}


akka {
  loglevel = INFO
  log-dead-letters = off
  log-dead-letters-during-shutdown = no

  cluster.failure-detector {

    # FQCN of the failure detector implementation.
    # It must implement akka.remote.FailureDetector and have
    # a public constructor with a com.typesafe.config.Config and
    # akka.actor.EventStream parameter.
    implementation-class = "akka.remote.PhiAccrualFailureDetector"

    # How often keep-alive heartbeat messages should be sent to each connection.
    heartbeat-interval = 4 s //1 s

    # Defines the failure detector threshold.
    # A low threshold is prone to generate many wrong suspicions but ensures
    # a quick detection in the event of a real crash. Conversely, a high
    # threshold generates fewer mistakes but needs more time to detect
    # actual crashes.
    threshold = 20.0   // 8.0

    # Number of the samples of inter-heartbeat arrival times to adaptively
    # calculate the failure timeout for connections.
    max-sample-size = 1000

    # Minimum standard deviation to use for the normal distribution in
    # AccrualFailureDetector. Too low standard deviation might result in
    # too much sensitivity for sudden, but normal, deviations in heartbeat
    # inter arrival times.
    min-std-deviation = 400 ms

    # Number of potentially lost/delayed heartbeats that will be
    # accepted before considering it to be an anomaly.
    # This margin is important to be able to survive sudden, occasional,
    # pauses in heartbeat arrivals, due to for example garbage collect or
    # network drop.
    acceptable-heartbeat-pause = 700 s  // 3 s

    # Number of member nodes that each member will send heartbeat messages to,
    # i.e. each node will be monitored by this number of other nodes.
    monitored-by-nr-of-members = 5

    # After the heartbeat request has been sent the first failure detection
    # will start after this period, even though no heartbeat message has
    # been received.
    expected-response-after = 4 s  // 1 s

  }

  actor {
    default-dispatcher {
      fork-join-executor {
        # Min number of threads to cap factor-based parallelism number to
        parallelism-min = 4

        # The parallelism factor is used to determine thread pool size using the
        # following formula: ceil(available processors * factor). Resulting size
        # is then bounded by the parallelism-min and parallelism-max values.
        parallelism-factor = 1.0

        # Max number of threads to cap factor-based parallelism number to
        parallelism-max = 12
      }
    }
  }

  kafka {
    producer {
      close-timeout = 60s
      kafka-clients = ${akka.kafka.consumer.kafka-clients}
    }
    consumer {
      kafka-clients {
        bootstrap.servers = ${adapt.env.kafkabootstrap}
        group.id = ADAPT-${adapt.runtime.systemname} // ${adapt.env.ta1}  //-${adapt.env.scenario}
        auto.offset.reset = "earliest"

        security.protocol = PLAINTEXT // SSL   // https://docs.confluent.io/current/kafka/authentication_ssl.html
      }
      wakeup-timeout = 10s
    }

    default-dispatcher {
      type = "Dispatcher"
      executor = "thread-pool-executor"
      thread-pool-executor {
        fixed-pool-size = 1
      }
    }
  }

  http {
    host-connection-pool.max-connections = 4096
    server {
      interface = ${adapt.runtime.webinterface}
      port = ${adapt.runtime.port}
      request-timeout = ${adapt.runtime.apitimeout} seconds
      idle-timeout = ${akka.http.server.request-timeout}
    }
  }
}


stream-dispatcher {
  type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-min = 0
    parallelism-max = 1
  }
}


quine {
  actor {
    graph-shard-dispatcher {
      type = Dispatcher
      throughput = 100
      executor = "thread-pool-executor"
      thread-pool-executor {
        core-pool-size-min = 1
        core-pool-size-factor = 1.0
        core-pool-size-max = 1
      }
    }

    persistor-blocking-dispatcher {
      type = Dispatcher
      throughput = 1
      executor = "thread-pool-executor"
      thread-pool-executor {
        fixed-pool-size = 2
      }
    }

    node-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      fork-join-executor {
        parallelism-min = 1
        parallelism-max = 2
      }
    }
  }
}
