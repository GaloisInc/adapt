adapt {
  runflow = "e3" // See: Application.scala
  ingest {

    // Old style for providing data: just a list of the files to ingest
    loadfiles = [] //"/Users/ryan/Desktop/ta1-cadets-pandex-cdm17.bin"] //ta1-trace-cdm17.bin" // ta1-clearscope-cdm17.bin" // cdm17_0407_1607.bin" //  ta1-clearscope-cdm17.bin"  //ta1-cadets-cdm17-3.bin" //]

    //data {
      //"" = ${adapt.ingest.loadfiles}
    //}
    // New style for providing data: the keys are provider names, the values are files for that provider
    data = [
//             {
//               provider=ta1-cadets-e3-official,
//               files = [
//                         /path/to/ta1-cadets-e3-official-1.bin,
//                         /path/to/ta1-cadets-e3-official-1.bin.1
//                       ]
//             }
           ]

    startatoffset = 0
    loadlimit = 0    // 0 == no limit
    quitafteringest = no
    logduplicates = no
    produceadm = true
    producecdm = false
  }
  runtime {
    port = 8080
    apitimeout = 601
    dbkeyspace = "neo4j"    // "neo4j" or "titan" is the default value if keyspace is never set
    neo4jkeyspace = ${adapt.runtime.dbkeyspace}
    neo4jfile = ${adapt.runtime.neo4jkeyspace}.db  // "neo4j.db"
    titankeyspace = ${adapt.runtime.dbkeyspace}   // "titan" is the default value if keyspace is never set
    titanthriftframesize = 200  // Default is 15: which definitely fails with normal queries. (we've observed at least 130mb in one frame)
    expansionqueryfreq = 60
    iforestpath = "/home/darpa/iforest.exe"  //  "/Users/ryan/Code/adapt/AdaptJVM/src/main/resources/bin/iforest.exe"  //
    iforestparallelism = 4
    shouldnoramlizeanomalyscores = false
    systemname = "Engagement3"
    quitonerror = no
  }
  env {
    ta1 = "file"
//    scenario = "not used"
    ta1kafkatopic = ta1-${adapt.env.ta1}-e3-official${adapt.env.topicsuffix}  // ta1-${adapt.env.ta1}-${adapt.env.scenario}-cdm17
    ta1kafkatopics = [${adapt.env.ta1kafkatopic}]

    theiaquerytopic = ta1-theia-adapt-q
    theiaresponsetopic = ta1-theia-adapt-qr
    theiaqueryidstart = 2000000

    topicsuffix = "" // one of: "-psa" "-psb" "-dift" or empty string so that no suffix is used with other TA1s
    kafkabootstrap =  "128.55.12.69:9092" // <--policy_demo    // E3: "128.55.12.59:9094"  //"ta3-starc-adapt-1-tcip.tc.bbn.com:9092"  //"localhost:9092"
    truststorepath = "/var/private/ssl/kafka.client.truststore.jks"
    trustpass = "TransparentComputing"
    keystorepath = "/var/private/ssl/kafka.client.keystore.jks"
    keypass = "TransparentComputing"
    sslkey = "TransparentComputing"
//    kafkasslenabled = yes
//    cdmavroschemafilepath = "/home/darpa/TCCDMDatum18.avsc"
  }

  // NOTE: these settings are only used if `adapt.ingest.produceadm` is `yes`
  adm {
    // Maximum time hop (in seconds) in observed tip-of-the-stream time (does not apply to TimeMarkers) that is allowed
    // when assigning timestamps to nodes in the graph
    maxtimejumpsecs = 1000000

    // When to expire CdmUUIDs in the UUID remap step expiry
    cdmexpiryseconds = 300
    cdmexpirycount = 100000

    // When to expire event chains in the event resolution step
    maxeventsmerged = 100    // Maximum number of CDM events that can be merged into a single ADM event
    eventexpirysecs = 10
    eventexpirycount = 10000

    // Elements in the LRU cache in the dedup step
//    dedupNodeCacheSize = 2000000
    dedupEdgeCacheSize = 2000000
    cdm2cdmlrucachesize = 10000000
    cdm2admlrucachesize = 30000000

    // Hack to keep down the size of mapDB - assumes that edges in the initial CDM data never point to event nodes
    ignoreeventremaps = false

    // Location of MapDB file DB. Uncomment this to have information about maps persisted on shutdown.
    mapdb = "map.db"
    mapdbbypasschecksum = no  // don't change this unless you are desperate and know what you are doing
    mapdbtransactions = no
  }

  logfile = "log.json.txt"

  ppm {
    saveintervalseconds = 1200
    pluckingdelay = 1000
    basedir = "/home/darpa/ppm/"   // MUST end in a slash
    eventtypemodelsdir = ${adapt.ppm.basedir}iforest/         // MUST end in a slash
    loadfilesuffix = -${adapt.env.ta1}
    savefilesuffix = ${adapt.ppm.loadfilesuffix}-save
    shouldload = yes
    shouldsave = yes
    rotatescriptpath = "/home/darpa/rotate-components.sh"

    components {
      events = ${adapt.ppm.basedir}event-components.json
      everything = ${adapt.ppm.basedir}everything-components.json
      pathnodes = ${adapt.ppm.basedir}pathnodes-components.json
      pathnodeuses = ${adapt.ppm.basedir}pathnodeuses-components.json
      releasequeue = ${adapt.ppm.basedir}releasequeue-components.json
    }

    iforestfreqminutes = 15
    iforesttrainingfile = train_iforest.csv
    iforesttrainingsavefile = train_iforest-UPDATED.csv
    iforestenabled = false
  }

  // These options are only used when the runflow is "accept"
  test {
    web-ui = true // Don't exit until user hits CTRL-C
  }
}

akka {
  loglevel = INFO
  kafka {
    producer {
      close-timeout = 60s
      kafka-clients = ${akka.kafka.consumer.kafka-clients}
    }
    consumer {
      kafka-clients {
        bootstrap.servers = ${adapt.env.kafkabootstrap}
        group.id = ADAPT-${adapt.runtime.systemname}-${adapt.env.ta1}  //-${adapt.env.scenario}
        auto.offset.reset = "earliest"

//        security.protocol = SSL   // https://docs.confluent.io/current/kafka/authentication_ssl.html
//        ssl.truststore.location = ${adapt.env.truststorepath}
//        ssl.truststore.password = ${adapt.env.trustpass}
//        ssl.keystore.location = ${adapt.env.keystorepath}
//        ssl.keystore.password = ${adapt.env.keypass}
//        ssl.key.password = ${adapt.env.sslkey}
      }
      wakeup-timeout = 10s
    }
  }
//  actor.default-dispatcher.throughput = 100000
  http.server {
    interface = "0.0.0.0"
    port = ${adapt.runtime.port}
    request-timeout = ${adapt.runtime.apitimeout} seconds
    idle-timeout = ${akka.http.server.request-timeout}
  }

  log-dead-letters-during-shutdown = no

//  actor.warn-about-java-serializer-usage = no
}


// See mailbox sizes
//akka.actor.default-mailbox {
//  mailbox-type = com.galois.adapt.LoggingMailboxType
//  size-limit = 1000
//}
