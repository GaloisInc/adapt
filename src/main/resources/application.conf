adapt {
  runflow = "ui"
  app = "prod"  // One of: `accept`, `dev`, `prod` or maybe `cluster`
  ingest {
    loadfiles = [] //"/Users/ryan/Desktop/ta1-cadets-pandex-cdm17.bin"] //ta1-trace-cdm17.bin" // ta1-clearscope-cdm17.bin" // cdm17_0407_1607.bin" //  ta1-clearscope-cdm17.bin"  //ta1-cadets-cdm17-3.bin" //]
    startatoffset = 0
    loadlimit = 0    // 0 == no limit
    parallelism = 1
    quitafteringest = yes
    logduplicates = no
    produceadm = true
    producecdm = false
  }
  runtime {
    port = 8080
    apitimeout = 301
    dbkeyspace = "neo4j"    // "neo4j" or "titan" is the default value if keyspace is never set
    neo4jkeyspace = ${adapt.runtime.dbkeyspace}
    neo4jfile = ${adapt.runtime.neo4jkeyspace}.db  // "neo4j.db"
    titankeyspace = ${adapt.runtime.dbkeyspace}   // "titan" is the default value if keyspace is never set
    titanthriftframesize = 200  // Default is 15: which definitely fails with normal queries. (we've observed at least 130mb in one frame)
    cleanupthreshold = 10  // after this many cleanup messages, the stream component will delete or write to disk
    expansionqueryfreq = 60
    iforestpath = "/home/darpa/iforest.exe"  //  "/Users/ryan/Code/adapt/AdaptJVM/src/main/resources/bin/iforest.exe"  //
    iforestparallelism = 4
    shouldnoramlizeanomalyscores = false
    basecleanupseconds = 10
    featureextractionseconds = 10
    throwawaythreshold = 10000
    notesfile = "/home/darpa/notes.json" //  "/Users/ryan/Desktop/notes.json"  //
    systemname = "Engagement2+"
  }
  env {
    ta1 = "file"
    scenario = "pandex"
    ta1kafkatopic = ta1-${adapt.env.ta1}-${adapt.env.scenario}-cdm17
    theiaresponsetopic = ta1-theia-${adapt.env.scenario}-qr
    kafkabootstrap = "ta3-starc-adapt-1-tcip.tc.bbn.com:9092"  //"localhost:9092"
  }

  // NOTE: these settings are only used if `adapt.ingest.produceadm` is `yes`
  adm {
    parallelism = 10000           // How many futures of resolved ADMs to similtaneously await
    timeoutSeconds = 21474830     // How long before the futures of resolved ADMs should timeout
  }
}

akka {
  loglevel = INFO
  kafka {
    producer {
      close-timeout = 60s
      kafka-clients {
        bootstrap.servers = ${adapt.env.kafkabootstrap}
      }
    }
    consumer {
      kafka-clients {
        bootstrap.servers = ${adapt.env.kafkabootstrap}
        group.id = ADAPT-${adapt.runtime.systemname}-${adapt.env.ta1}-${adapt.env.scenario}
      }
    }
  }
  http.server {
    interface = "0.0.0.0"
    port = ${adapt.runtime.port}
    request-timeout = ${adapt.runtime.apitimeout} seconds
    idle-timeout = ${akka.http.server.request-timeout}
  }


  actor.warn-about-java-serializer-usage = no


//  actor {
//    provider = "cluster"
//  }
//  version = 2.4.18   // Only needed for clusering... I think.
//  cluster {
//    protocol-prefix = "akka.tcp://"
//    seed-nodes = [
//      "akka.tcp://"${adapt.runtime.systemname}"@127.0.0.1:2551"
////    , "akka.tcp://"${adapt.runtime.systemname}"@127.0.0.1:2552"
//    ]
////    this-node-address = ${akka.cluster.protocol-prefix}${adapt.runtime.systemname}"@"${akka.remote.netty.tcp.address}":"${akka.remote.netty.tcp.port}
//    roles = ${adapt.roles}
//    metrics.enabled=off
//    min-nr-of-members = 2
//  }
//  remote {
//    log-remote-lifecycle-events = off
//    netty.tcp {
//      hostname = "127.0.0.1"
//      port = 2551
//      message-frame-size =  1000000b
//      send-buffer-size =  1000000b
//      receive-buffer-size =  1000000b
//      maximum-frame-size = 1000000b
//    }
//    maximum-payload-bytes = 1000000 bytes
//  }
//  persistence {
//    journal.plugin = "inmemory-journal"
//    snapshot-store.plugin = "inmemory-snapshot-store"
//  }

}

