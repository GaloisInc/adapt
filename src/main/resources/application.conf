adapt {
  runflow = "ui" // One of: `ui`, `db`, `csv`, `accept`, etc.  See: Application.scala
  ingest {

    // Old style for providing data: just a list of the files to ingest
    loadfiles = [] //"/Users/ryan/Desktop/ta1-cadets-pandex-cdm17.bin"] //ta1-trace-cdm17.bin" // ta1-clearscope-cdm17.bin" // cdm17_0407_1607.bin" //  ta1-clearscope-cdm17.bin"  //ta1-cadets-cdm17-3.bin" //]

    // New style for providing data: the keys are provider names, the values are files for that provider
    data {
      "" = ${adapt.ingest.loadfiles}
    }

    startatoffset = 0
    loadlimit = 0    // 0 == no limit
    quitafteringest = no
    logduplicates = no
    produceadm = true
    producecdm = false
  }
  runtime {
    port = 8080
    apitimeout = 301
    dbkeyspace = "neo4j"    // "neo4j" or "titan" is the default value if keyspace is never set
    neo4jkeyspace = ${adapt.runtime.dbkeyspace}
    neo4jfile = ${adapt.runtime.neo4jkeyspace}.db  // "neo4j.db"
    titankeyspace = ${adapt.runtime.dbkeyspace}   // "titan" is the default value if keyspace is never set
    titanthriftframesize = 200  // Default is 15: which definitely fails with normal queries. (we've observed at least 130mb in one frame)
    expansionqueryfreq = 60
    iforestpath = "/home/darpa/iforest.exe"  //  "/Users/ryan/Code/adapt/AdaptJVM/src/main/resources/bin/iforest.exe"  //
    iforestparallelism = 4
    shouldnoramlizeanomalyscores = false
    systemname = "Engagement3"
    quitonerror = no
  }
  env {
    ta1 = "file"
    scenario = "not used"
    ta1kafkatopic = ta1-${adapt.env.ta1}-e3${adapt.env.topicsuffix}  // ta1-${adapt.env.ta1}-${adapt.env.scenario}-cdm17

    theiaresponsetopic = ta1-theia-e3-qr  // TODO: NEED TO CONFIRM WHICH TOPIC TO USE!!!!!!!!!!!!!
    theiaqueryidstart = 2000000

    topicsuffix = "" // one of: "-psa" "-psb" "-dift" or empty string so that no suffix is used with other TA1s
    kafkabootstrap = "128.55.12.59:9094"  //"ta3-starc-adapt-1-tcip.tc.bbn.com:9092"  //"localhost:9092"
    truststorepath = "/var/private/ssl/kafka.client.truststore.jks"
    trustpass = "TransparentComputing"
    keystorepath = "/var/private/ssl/kafka.client.keystore.jks"
    keypass = "TransparentComputing"
    sslkey = "TransparentComputing"
    //    kafkasslenabled = yes
    cdmavroschemafilepath = "/home/darpa/TCCDMDatum18.avsc"
  }

  // NOTE: these settings are only used if `adapt.ingest.produceadm` is `yes`
  adm {
    // Maximum time hop (in seconds) in observed tip-of-the-stream time (does not apply to TimeMarkers) that is allowed
    // when assigning timestamps to nodes in the graph
    maxtimejumpsecs = 100

    // When to expire CdmUUIDs in the UUID remap step expiry
    cdmexpiryseconds = 600
    cdmexpirycount = 10000

    // When to expire event chains in the event resolution step
    maxeventsmerged = 100    // Maximum number of CDM events that can be merged into a single ADM event
    eventexpirysecs = 10
    eventexpirycount = 10000

    // Elements in the LRU cache in the dedup step
    dedupNodeCacheSize = 2000000
    dedupEdgeCacheSize = 2000000
  }

  logfile = "log.csv"

  ppm {
    saveintervalseconds = 60
    basedir = "/Users/ryan/Desktop/ppm_experiments/"

//  ------ BEGIN EXAMPLE: ------
//    PpmTreeName {  // Case and spelling must match the hardcoded string in PPMTree.scala
//      loadfile = "/path/to/load/from"
//      savefile = "/path/to/save/to"
//    }
//  ------ END EXAMPLE ---------

    ProcessFileTouches {
//      loadfile = "ProcessFileTouches.csv"
//      loadfile =
      savefile =
        "ProcessFileTouches-Bovia.csv"
    }
    FilesTouchedByProcesses {
//      loadfile = "FilesTouchedByProcesses.csv"
//      loadfile =
      savefile =
        "FilesTouchedByProcesses-Bovia.csv"
    }
    FilesExecutedByProcesses {
//      loadfile = "FilesExecutedByProcesses.csv"
//      loadfile =
      savefile =
        "FilesExecutedByProcesses-Bovia.csv"
    }
    ProcessesWithNetworkActivity {
//      loadfile = "ProcessesWithNetworkActivity.csv"
//      loadfile =
      savefile =
        "ProcessesWithNetworkActivity-Bovia.csv"
    }
    DirectoryStructure {
//      loadfile = "DirectoryStructure.csv"
//      loadfile =
      savefile =
        "DirectoryStructure-Bovia.csv"
    }
    ProcessDirectoryTouchesV1 {
//      loadfile = "ProcessDirectoryTouches.csv"
//      loadfile =
      savefile =
        "ProcessDirectoryTouches-Bovia.csv"
    }
    ProcessDirectoryTouchesV2 {

    }
    ProcessEventType {
      loadfile = "ProcessEventType.csv"
      savefile = "ProcessEventType2.csv"
    }
    eventtypemodelsdir = "/Users/nls/Desktop/ppm_experiments/model/iforest/"
    processdirectorytouchesauxfile = "model_file_touch.json"
  }
}

akka {
  loglevel = INFO
  kafka {
    producer {
      close-timeout = 60s
      kafka-clients = ${akka.kafka.consumer.kafka-clients}
    }
    consumer {
      kafka-clients {
        bootstrap.servers = ${adapt.env.kafkabootstrap}
        group.id = ADAPT-${adapt.runtime.systemname}-${adapt.env.ta1}  //-${adapt.env.scenario}
        auto.offset.reset = "earliest"

        security.protocol = SSL   // https://docs.confluent.io/current/kafka/authentication_ssl.html
        ssl.truststore.location = ${adapt.env.truststorepath}
        ssl.truststore.password = ${adapt.env.trustpass}
        ssl.keystore.location = ${adapt.env.keystorepath}
        ssl.keystore.password = ${adapt.env.keypass}
        ssl.key.password = ${adapt.env.sslkey}
      }
    }
  }
//  actor.default-dispatcher.throughput = 100000
  http.server {
    interface = "0.0.0.0"
    port = ${adapt.runtime.port}
    request-timeout = ${adapt.runtime.apitimeout} seconds
    idle-timeout = ${akka.http.server.request-timeout}
  }

  log-dead-letters-during-shutdown = no

//  actor.warn-about-java-serializer-usage = no
}


// See mailbox sizes
//akka.actor.default-mailbox {
//  mailbox-type = com.galois.adapt.LoggingMailboxType
//  size-limit = 1000
//}
