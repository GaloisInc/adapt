adapt {
  runflow = "e4" // See: Application.scala
  ingest {
    hosts = [
      {
        ta1 = FiveDirections
        hostname = "FiveDirections-A"
        parallel = [
          {
            sequential = [
              {
                type = kafka
                topicname = "ta1-fivedirections-benign-bgt-e4-A"
                namespace = "5d-host-a-benign"
              },
              {
                type = kafka
                topicname = "ta1-fivedirections-e4-A"
                namespace = "5d-host-a"
              }
            ]
          }
        ]
      },
      {
        ta1 = FiveDirections
        hostname = "FiveDirections-B"
        parallel = [
          {
            sequential = [
              {
                type = kafka
                topicname = "ta1-fivedirections-benign-bgt-e4-B"
                namespace = "5d-host-b-benign"
              },
              {
                type = kafka
                topicname = "ta1-fivedirections-e4-B"
                namespace = "5d-host-b"
              }
            ]
          }
        ]
      }
    ]

    quitafteringest = yes
    logduplicates = no
    produce = produceadm
  }

  runtime {
    webinterface = "0.0.0.0"
    port = 8080
    apitimeout = 601
    dbkeyspace = "neo4j"    // "neo4j" or "titan" is the default value if keyspace is never set
    neo4jkeyspace = ${adapt.runtime.dbkeyspace}
    neo4jfile = ${adapt.runtime.neo4jkeyspace}.db  // "neo4j.db"
    systemname = "Engagement4"
    quitonerror = no
    logfile = "log.json.txt"
  }

  env {
    kafkabootstrap = "128.55.12.59:9094"  //"128.55.12.69:9092"   // E3: "128.55.12.59:9094"  //"ta3-starc-adapt-1-tcip.tc.bbn.com:9092"  //"localhost:9092"
    truststorepath = "/var/private/ssl/kafka.client.truststore.jks"
    trustpass = "TransparentComputing"
    keystorepath = "/var/private/ssl/kafka.client.keystore.jks"
    keypass = "TransparentComputing"
    sslkey = "TransparentComputing"
//    kafkasslenabled = yes
//    cdmavroschemafilepath = "/home/darpa/TCCDMDatum18.avsc"
  }

  // NOTE: these settings are only used if `adapt.ingest.produceadm` is `yes`
  adm {
    // Maximum time hop (in seconds) in observed tip-of-the-stream time (does not apply to TimeMarkers) that is allowed
    // when assigning timestamps to nodes in the graph
    maxtimejumpsecs = 1000000

    // When to expire CdmUUIDs in the UUID remap step expiry
    cdmexpiryseconds = 300
    cdmexpirycount = 100000

    // When to expire event chains in the event resolution step
    maxeventsmerged = 100    // Maximum number of CDM events that can be merged into a single ADM event
    eventexpirysecs = 10
    eventexpirycount = 10000

    // Elements in the LRU cache in the dedup step
    dedupedgecachesize = 2000000

    uuidremappershards = 4           // Setting this to 0 uses the old (pre-sharding) mechanism.
    cdm2cdmlrucachesize = 10000000
    cdm2admlrucachesize = 30000000

    // Hack to keep down the size of mapDB - assumes that edges in the initial CDM data never point to event nodes
    ignoreeventremaps = false

    // Location of MapDB file DB. Uncomment this to have information about maps persisted on shutdown.
//    mapdb = ${adapt.runtime.neo4jfile}/map.db  // leaving this absent will create a temporary file instead
    mapdbbypasschecksum = no  // don't change this unless you are desperate and know what you are doing
    mapdbtransactions = no
  }



  ppm {
//    saveintervalseconds = 1200
    pluckingdelay = 10000
    basedir = "ppm/"   // MUST end in a slash
    eventtypemodelsdir = ${adapt.ppm.basedir}iforest/   // MUST end in a slash
//    loadfilesuffix = "-save" // ${adapt.env.ta1}
    savefilesuffix = -save   //${adapt.ppm.loadfilesuffix}-save

    shouldloadppmtree = yes
    shouldloadalarms = no
    shouldloadlocalprobabilitiesfromalarms = yes
    shouldloadppmpartialobservationaccumulators = no

    shouldsaveppmtree = yes
    shouldsavealarms = yes
    shouldsaveppmpartialobservationaccumulators = no

    rotatescriptpath = "" // /home/darpa/rotate-components.sh"

    components {
      events = ${adapt.ppm.basedir}event-components.json
      everything = ${adapt.ppm.basedir}everything-components.json
      pathnodes = ${adapt.ppm.basedir}pathnodes-components.json
      pathnodeuses = ${adapt.ppm.basedir}pathnodeuses-components.json
      releasequeue = ${adapt.ppm.basedir}releasequeue-components.json
    }

    iforestfreqminutes = 15
    iforesttrainingfile = train_iforest.csv
    iforesttrainingsavefile = train_iforest-UPDATED.csv
    iforestenabled = false
    
    computethresholdintervalminutes = 15,
    alarmlppercentile = 0.1
  }

  // These options are only used when the runflow is "accept"
  test {
    web-ui = true // Don't exit until user hits CTRL-C
  }

  alarms {

    splunk {
      enabled = true
      token = 4265eb49-40ff-4bb1-a096-e9f4a51e6d17
      host = "ta2-adapt-7"
      port = 8088
      detailed-reporting-period-seconds = 900
      realtime-reporting-period-seconds = 1
      maxbufferlength = 500
    }

    logging {
      enabled = false
    }

    console {
      enabled = false
    }

    gui {
      enabled = false
    }
  }
}

akka {
  loglevel = INFO
  kafka {
    producer {
      close-timeout = 60s
      kafka-clients = ${akka.kafka.consumer.kafka-clients}
    }
    consumer {
      kafka-clients {
        bootstrap.servers = ${adapt.env.kafkabootstrap}
        group.id = ADAPT-${adapt.runtime.systemname} // ${adapt.env.ta1}  //-${adapt.env.scenario}
        auto.offset.reset = "earliest"

        security.protocol = SSL   // https://docs.confluent.io/current/kafka/authentication_ssl.html
        ssl.truststore.location = ${adapt.env.truststorepath}
        ssl.truststore.password = ${adapt.env.trustpass}
        ssl.keystore.location = ${adapt.env.keystorepath}
        ssl.keystore.password = ${adapt.env.keypass}
        ssl.key.password = ${adapt.env.sslkey}
      }
      wakeup-timeout = 10s
    }
  }
//  actor.default-dispatcher.throughput = 100000
  http.server {
    interface = ${adapt.runtime.webinterface}
    port = ${adapt.runtime.port}
    request-timeout = ${adapt.runtime.apitimeout} seconds
    idle-timeout = ${akka.http.server.request-timeout}
  }

  log-dead-letters-during-shutdown = no

//  actor.warn-about-java-serializer-usage = no

  akka.http.host-connection-pool {
    max-connections = 4096
  }
}


// See mailbox sizes
//akka.actor.default-mailbox {
//  mailbox-type = com.galois.adapt.LoggingMailboxType
//  size-limit = 1000
//}
